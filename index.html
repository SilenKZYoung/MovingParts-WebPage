<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="">
  <meta name="author" content="">
  <link href="css/bootstrap.min.css" rel="stylesheet">

  <title>MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field</title>
</head>

<body>
  <!-- Page Content -->
  <div class="container">
    <div class="row">
      <div class="col-lg-12">
        <div>
          <div class='row mt-5 mb-3'>
              <div class='col text-center'>
                  <p class="h2 font-weight-normal">MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field</p>
              </div>
          </div>

          <!-- authors -->
          <div class='mb-4 h5 text-center'>
              <a class="col-md-3 col-xs-6" href="https://silenkzyoung.github.io/KaizhiYang/"><span>Kaizhi Yang<sup>1</sup></span></a>
              <a class="col-md-3 col-xs-6" href="https://i.buriedjet.com/"><span>Xiaoshuai Zhang<sup>2</sup></span></a>
              <a class="col-md-3 col-xs-6" href="https://sites.google.com/view/zhiao-huang"><span>Zhiao Huang<sup>2</sup></span></a>
              <a class="col-md-3 col-xs-6" href="http://staff.ustc.edu.cn/~xjchen99/"><span>Xuejin Chen<sup>1</sup></span></a>
              <a class="col-md-3 col-xs-6" href="https://cseweb.ucsd.edu/~zex014/"><span>Zexiang Xu<sup>3</sup></span></a>
              <a class="col-md-3 col-xs-6" href="https://cseweb.ucsd.edu/~haosu/index.html"><span>Hao Su<sup>2</sup></span></a>
          </div>

          <!-- affiliations -->
          <div class='row mt-1 mt-2' >
              <div class='col text-center'>
                  <p class="h6 font-weight-light">
                  <a class="ml-4" href="https://www.ustc.edu.cn/"><span> <sup>1</sup> University of Science and Technology of China</span></a>
                  <a class="ml-4" href="https://ucsd.edu//"><span> <sup>2</sup> University of California, San Diego</span></a>
                  <a class="mr-4 ml-4" href="https://research.adobe.com/"><span> <sup>3</sup> Adobe Research</span></a>
                  </p>
              </div>
          </div>

          <!-- paper code -->
          <div class='row mt-1 mt-2' >
            <div class='col text-center'>
                <!-- <p class="h6 font-weight-light"> -->
                <a class="ml-4" href="https://arxiv.org/abs/2303.05703"><span>[Paper]</span></a>
                <a class="ml-4" href="https://github.com/SilenKZYoung/MovingParts"><span>[Code]</span></a>
                </p>
            </div>
        </div>

        <br />

        
      <!-- Abstract section -->
      <div>
          <div class='row'>
              <div class="container" id="abstract">
                <h2>Abstract</h2>
                <p style="text-align:justify">
                  We present MovingParts, a NeRF-based method for dynamic scene reconstruction and part discovery. We consider motion as an important cue for identifying parts, that all particles on the same part share the common motion pattern. From the perspective of fluid simulation, existing deformation-based methods for dynamic NeRF can be seen as parameterizing the scene motion under the Eulerian view, i.e., focusing on specific locations in space through which the fluid flows as time passes. However, it is intractable to extract the motion of constituting objects or parts using the Eulerian view representation. In this work, we introduce the dual Lagrangian view and enforce representations under the Eulerian/Lagrangian views to be cycle-consistent. Under the Lagrangian view, we parameterize the scene motion by tracking the trajectory of particles on objects. The Lagrangian view makes it convenient to discover parts by factorizing the scene motion as a composition of part-level rigid motions. Experimentally, our method can achieve fast and high-quality dynamic scene reconstruction from even a single moving camera, and the induced part-based representation allows direct applications of part tracking, animation, 3D scene editing, etc.
                </p>
              </div>
          </div>
      </div>

      <br />

      <div>
        <div class='row'>
            <div class="container" id="motivation">
              <h2>Motivation</h2>
              <p style="text-align:justify">
                Inspired by the Eulerian and Lagrangian viewpoints in fluid simulation, the Eulerian view observes the flow at a specific location and the Lagrangian view observes the trajectory of specific particles. These two views constitute the conversion between the world space of each temporal frame and a canonical space.
                Previously, deformation-based dynamic NeRF relied heavily on the Eulerian perspective. While this approach enables the modeling of entire scene motion, analyzing object/part motion at different temporal moments can be impractical and challenging.
                In contrast, the Lagrangian perspective employs particle-based representation to trace the movement of each object particle, and the resulting trajectory offers valuable insights for scene analysis.
                Here we propose a hybrid approach that leverages both Eulerian and Lagrangian views to achieve high-quality reconstruction and meaningful scene understanding simultaneously. 
              </p>
            </div>
            <div class="col text-center">
              <img src="videos/motivation.png" width="85%"></img>
            </div>
        </div>
      </div>

      <br />
      <br />

      <div>
        <div class='row'>
            <div class="container" id="network">
              <h2>Network Architecture</h2>
              <p style="text-align:justify">
                Our network contains three modules. The Eulerian module and the Lagrangian module observe the motion of specific spatial positions and specific particles, respectively. They both comprise a mutual mapping of a point between its position at an arbitrary time instance and its canonical configuration. The canonical module serves to reconstruct the geometry and appearance for volume rendering. Based on the particle trajectories recorded by the Lagrangian module, we can analyze the motion patterns and discover rigid parts using a motion grouping network.
              </p>
            </div>
            <div class="col text-center">
              <img src="videos/network.png" width="85%"></img>
            </div>
        </div>
      </div>

      <br />
      <br />

    <!-- results section -->
    <div>
        <div class='row'>
            <div class="container" id="results">
              <h2>Results on D-NeRF Synthetic Dataset</h2>
              <p style="text-align:justify">
                Our method enables high-quality appearance and geometry reconstruction for dynamic scenes. Also thanks to the motion-based grouping mechanism, our method is capable of overlooking motion-irrelevant characteristics in geometry and appearance and produce clean and accurate part discovery results on these realistic complex scenes.
              </p>
            </div>
            <div class="n-page video">
              <video class="centered" width="100%" autoplay loop muted playsinline>
                  <source src="videos/dnerf.mp4#t=0.001" type="video/mp4" />
              </video>
            </div>
            </div>
        </div>
    </div>

    <br />
    <br />

    <div>
      <div class='row'>
          <div class="container" id="results">
            <h2>Application: Robotic Manipulation</h2>
            <p style="text-align:justify">
              Our method can discover the operated parts, such as the drawer (orange), even it is largely occluded in most frames. The discovered 3D parts with their Lagrangian motion could serve as a strong prior for downstream functionality reasoning and robotic reinforcement learning tasks.
            </p>
          </div>
          <div class="n-page video">
            <video class="centered" width="100%" autoplay loop muted playsinline>
                <!-- t=0.001 is a hack to make iPhone show video thumbnail -->
                <source src="videos/manipulation.mp4#t=0.001" type="video/mp4" />
            </video>
          </div>
      </div>
  </div>

  <br />
  <br />

  <div>
    <div class='row'>
        <div class="container" id="results">
          <h2>Application: Scene Editing</h2>
          <p style="text-align:justify">
            In addition to understanding scenes, our method can also edit scenes and generate new renderings from the scene. Here, we show the dense tracking, duplication, deletion, scaling of a specific part in a scene, and re-posing of the articulated robot arm. Also we demonstrate the scalability of our method to real-world scenarios by showcasing the removal or modification of the position of specific objects in real scenes.
          </p>
        </div>
        <div class="col text-center">
          <img src="videos/editing.png" width="100%"></img>
        </div>
    </div>
  </div>

  <br />
  <br />

  <div>
    <div class='row'>
      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <h2 class="title">BibTeX</h2>
          <pre><code>@article{movingparts2023,
            title={MovingParts: Motion-based 3D Part Discovery in Dynamic Radiance Field},
            author={Kaizhi Yang and Xiaoshuai Zhang and Zhiao Huang and Xuejin Chen and Zexiang Xu and Hao Su},
            journal={arXiv:2303.05703},
            year=2023
        }
    </code></pre>
    </div>
    
  </div>

  <br />
  <br />
  

      </div>
    </div>
  </div>

</body>

</html>